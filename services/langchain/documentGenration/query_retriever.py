import os
from langchain_core.prompts import ChatPromptTemplate
from langchain_community.vectorstores import Chroma
from langchain.prompts import ChatPromptTemplate
from langchain_community.llms.ollama import Ollama
from services.langchain.embedding import get_embedding_function
from fastapi import status, HTTPException
from fastapi.responses import JSONResponse
# from chromadb.utils import parse_query_string
from utilservice import *


# Path to the Chroma vector store
CHROMA_PATH = "././chroma/langchain"


# Template for generating prompts to query the language model
PROMPT_TEMPLATE = """

CONTEXT:
{context}

---

PROMPT:
{question}


Instruction:
This Prompt is AI/User Generated Prompt. Generate Response from the Context for This Prompt. Response Must be given from the Context.
The Transcript is Generated by Code, while the Response is Crafted in Natural Human Language.
Don't Hallucination Response. Response Language must be same as Context Language.  

"""



# Function to run a query against the Chroma vector store and return a response from the language model
def run_query(llm_model: str, prompt_obj: dict, project_id: str, recording_id: str):
    path = f"{CHROMA_PATH}/{project_id}"
    if not os.path.exists(path):
        print("Database not found")
        raise HTTPException(status_code=status.HTTP_404_NOT_FOUND, detail= f"Project_id '{project_id}' Not Found")
    
    embedding_function = get_embedding_function()
    db = Chroma(persist_directory=f"{CHROMA_PATH}/{project_id}", embedding_function=embedding_function)
    no_of_result = db._collection.count()
    response_obj = {}
    print(prompt_obj)
    for category, prompt in prompt_obj.items():
        results = db.similarity_search_with_score(prompt, k=5, filter={"recording_id": recording_id})
        print(results)

        context_text = "\n\n---\n\n".join([doc.page_content for doc, score in results])
        
        if context_text != "":        
            prompt_template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)
            prompt = prompt_template.format(context=context_text, question=prompt)
            llm_model = verify_llm(llm_model)  
            model = Ollama(model=llm_model, keep_alive = -1)
            
            response_text = model.invoke(prompt)
            language_code, language_name = language_detaction(response_text)
        
        elif context_text == "":
            response_text = "Context is Not Provided."
            language_name = "Not Detected"

        response = {
            "answer": response_text,
            "language": language_name
        }
        response_obj[category] = response

    return JSONResponse(content=response_obj, status_code=status.HTTP_200_OK)


